{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Second_Assignement.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOM2WfpgZoeD+Y6PDqdg/NA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoMinchio/NLU_Assignement_2/blob/main/Second_Assignement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBJf_5voQu9k"
      },
      "source": [
        "# SECOND_ASSIGNEMENT\n",
        "\n",
        "*   Student name: Francesco Minchio\n",
        "*   Student contact: francesco.minchio@studenti.unitn.it\n",
        "*   Student referal: 225269"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIzrWACvQ7bX"
      },
      "source": [
        "### **Basic imports**\n",
        "\n",
        "* en_core_web_sm with spacy.load (text processing)\n",
        "* panda (to be assessed)\n",
        "* Conll.py module\n",
        "* Read_corpus_conl function to load our dataset: this function in summary serves to store each sentence in a list of tuples, each containing a single line of text, with their respective tokens and labels. For this reason a list of phrases and labels are created. For each obtained line a string is created to store token text within the sentence being observed and a list is created to store pairs for each tuple.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1bdPSr9Mfww"
      },
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp_standard = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def read_corpus_conll(corpus_file, fs=\"\\t\"):\n",
        "    featn = None  # number of features for consistency check\n",
        "    sents = []  # list to hold words list sequences\n",
        "    words = []  # list to hold feature tuples\n",
        "\n",
        "    for line in open(corpus_file):\n",
        "        line = line.strip()\n",
        "        if len(line.strip()) > 0:\n",
        "            feats = tuple(line.strip().split(fs))\n",
        "            if not featn:\n",
        "                featn = len(feats)\n",
        "            elif featn != len(feats) and len(feats) != 0:\n",
        "                raise ValueError(\"Unexpected number of columns {} ({})\".format(len(feats), featn))\n",
        "\n",
        "            words.append(feats)\n",
        "        else:\n",
        "            if len(words) > 0:\n",
        "                sents.append(words)\n",
        "                words = []\n",
        "    return sents"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU_3BUE2Sl2z"
      },
      "source": [
        "### **TASK 0:**\n",
        "\n",
        "* Evaluate spaCy NER on CoNLL 2003 data (provided).\n",
        "\n",
        "Features:\n",
        "\n",
        "* WhitespaceTokenizer: we are able to extract the tokens from string of words or sentences without whitespaces, new line and tabs and produce a return of tokens from a string;\n",
        "* Spacy vocab.\n",
        "\n",
        "Next process: mapping to make spacy tags in conll tags:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OYcJZpKRUX5"
      },
      "source": [
        "class WhitespaceTokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __call__(self, text):\n",
        "        words = text.split(\" \")\n",
        "        return Doc(self.vocab, words=words)\n",
        "\n",
        "\n",
        "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\n",
        "\n",
        "spacy_to_conll_map = {\n",
        "    \"PERSON\": \"PER\",\n",
        "    \"NORP\": \"MISC\",\n",
        "    \"FACILITY\": \"ORG\",\n",
        "    \"FAC\": \"MISC\",\n",
        "    \"ORG\": \"ORG\",\n",
        "    \"GPE\": \"LOC\",\n",
        "    \"LOC\": \"LOC\",\n",
        "    \"PRODUCT\": \"MISC\",\n",
        "    \"EVENT\": \"MISC\",\n",
        "    \"WORK_OF_ART\": \"MISC\",\n",
        "    \"LAW\": \"MISC\",\n",
        "    \"LANGUAGE\": \"MISC\",\n",
        "    \"DATE\": \"MISC\",\n",
        "    \"TIME\": \"MISC\",\n",
        "    \"PERCENT\": \"MISC\",\n",
        "    \"MONEY\": \"MISC\",\n",
        "    \"QUANTITY\": \"MISC\",\n",
        "    \"ORDINAL\": \"MISC\",\n",
        "    \"CARDINAL\": \"MISC\",\n",
        "    \"PER\": \"PER\",\n",
        "    \"MISC\": \"MISC\",\n",
        "    \"EVT\": \"MISC\",\n",
        "    \"PROD\": \"MISC\",\n",
        "    \"DRV\": \"MISC\",\n",
        "    \"GPE_LOC\": \"LOC\",\n",
        "    \"GPE_ORG\": \"ORG\",\n",
        "    \"\": \"\"\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f83YR1FKUeOH"
      },
      "source": [
        "### **TASK 0.1:**\n",
        "\n",
        "* Report token-level performance (per class and total): accuracy of correctly recognizing all tokens that belong to named entities (i.e. tag-level accuracy)\n",
        "* Report CoNLL chunk-level performance (per class and total): precision, recall, f-measure of correctly recognizing all the named entities in a chunk per class and total\n",
        "\n",
        "Function: token_level_performance which contains the algorithm for this operation.\n",
        "By using read_corpus_conll, the word dataset contained in conll is read, while conll_data is a list of lists divided into two sections: externally there is the list of sentences ( sentence n), internally there is a list corresponding to a line of the dataset.\n",
        "In this way, by applying the algorithm, sentences are created and inserted in a string where each token is separated by \" \".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqoULwoyNLzP"
      },
      "source": [
        "def token_level_performance(conll_data):   #read the list of lists\n",
        "    total_tokens = 0\n",
        "    correctly_classified = 0\n",
        "    for sentence in conll_data:\n",
        "        token_array = []\n",
        "        part_of_speech_tag_array = []\n",
        "        chunck_tag_array = []\n",
        "        named_entity_tag_array = []\n",
        "        for element in sentence:\n",
        "            token = element[0].split()[0]\n",
        "            part_of_speech_tag = element[0].split()[1]\n",
        "            chunck_tag = element[0].split()[2]\n",
        "            named_entity_tag = element[0].split()[3]\n",
        "            token_array.append(token)\n",
        "            part_of_speech_tag_array.append(part_of_speech_tag)\n",
        "            chunck_tag_array.append(chunck_tag)\n",
        "            named_entity_tag_array.append(named_entity_tag)\n",
        "        doc = nlp(\" \".join(token_array))\n",
        "        token_index = 0\n",
        "        for token in doc:\n",
        "            total_tokens += 1\n",
        "            ent_type_converted_to_conll = token.ent_iob_\n",
        "            if(spacy_to_conll_map[token.ent_type_] != \"\"):\n",
        "                ent_type_converted_to_conll += \"-\" + \\\n",
        "                    spacy_to_conll_map[token.ent_type_]\n",
        "            if(ent_type_converted_to_conll == named_entity_tag_array[token_index]):\n",
        "                correctly_classified += 1\n",
        "            token_index += 1\n",
        "    print(total_tokens, correctly_classified)\n",
        "    return correctly_classified / total_tokens"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExvQCAteYovs"
      },
      "source": [
        "### **TASK 0.2:**\n",
        "\n",
        "* Report CoNLL chunk-level performance (per class and total);\n",
        "* Precision, recall, f-measure of correctly recognizing all the named entities in a chunk per class and total.\n",
        "\n",
        "A token array is provided by Conll through which the Chunks are rebuilt. The Chunk is recognized directly by the algorithm and checks if the label is correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXlVoh6kNIhN"
      },
      "source": [
        "def chunk_level_performance(conll_data):\n",
        "    effective_class_counts = {\n",
        "        \"MISC\": 0,\n",
        "        \"ORG\": 0,\n",
        "        \"PER\": 0,\n",
        "        \"LOC\": 0,\n",
        "    }\n",
        "    recognized_class_counts = {\n",
        "        \"MISC\": 0,\n",
        "        \"ORG\": 0,\n",
        "        \"PER\": 0,\n",
        "        \"LOC\": 0,\n",
        "    }\n",
        "    counter = 0\n",
        "    chunk_counter = 0\n",
        "    recognized_chunk_counter = 0\n",
        "    for sentence in conll_data:\n",
        "        token_array = []\n",
        "        part_of_speech_tag_array = []\n",
        "        chunck_tag_array = []\n",
        "        named_entity_tag_array = []\n",
        "        for element in sentence:\n",
        "            token = element[0].split()[0]\n",
        "            part_of_speech_tag = element[0].split()[1]\n",
        "            chunck_tag = element[0].split()[2]\n",
        "            named_entity_tag = element[0].split()[3]\n",
        "            token_array.append(token)\n",
        "            part_of_speech_tag_array.append(part_of_speech_tag)\n",
        "            chunck_tag_array.append(chunck_tag)\n",
        "            named_entity_tag_array.append(named_entity_tag)\n",
        "        doc = nlp(\" \".join(token_array))\n",
        "        actual_chunks, actual_chunk_label_array, non_empty_chunks, current_effective_class_counts = get_chunks(\n",
        "            token_array, named_entity_tag_array)\n",
        "        effective_class_counts[\"MISC\"] += current_effective_class_counts[\"MISC\"]\n",
        "        effective_class_counts[\"ORG\"] += current_effective_class_counts[\"ORG\"]\n",
        "        effective_class_counts[\"PER\"] += current_effective_class_counts[\"PER\"]\n",
        "        effective_class_counts[\"LOC\"] += current_effective_class_counts[\"LOC\"]\n",
        "        chunk_counter += non_empty_chunks\n",
        "        for ent in doc.ents:\n",
        "            if ent.text in actual_chunks:\n",
        "                recognized_chunk_counter += 1\n",
        "                actual_chunk_index = actual_chunks.index(ent.text)\n",
        "                if spacy_to_conll_map[ent.label_] == actual_chunk_label_array[actual_chunk_index]:\n",
        "                    recognized_class_counts[actual_chunk_label_array[actual_chunk_index]] += 1\n",
        "    return effective_class_counts, recognized_class_counts, chunk_counter, recognized_chunk_counter\n",
        "\n",
        "\n",
        "def get_chunks(token_array, named_entity_tag_array):\n",
        "    effective_class_counts = {\n",
        "        \"MISC\": 0,\n",
        "        \"ORG\": 0,\n",
        "        \"PER\": 0,\n",
        "        \"LOC\": 0,\n",
        "    }\n",
        "    chunk_array = []\n",
        "    chunk_label_array = []\n",
        "    current_chunk = \"\"\n",
        "    last_iob = \"\"\n",
        "    total_chunks = 0\n",
        "    for token_index, token in enumerate(token_array):\n",
        "        if named_entity_tag_array[token_index][0] == \"B\":\n",
        "            effective_class_counts[named_entity_tag_array[token_index][2:]] += 1\n",
        "            total_chunks += 1\n",
        "            chunk_label_array.append(named_entity_tag_array[token_index][2:])\n",
        "            if current_chunk != \"\":\n",
        "                chunk_array.append(current_chunk)\n",
        "            current_chunk = token\n",
        "        if named_entity_tag_array[token_index][0] == \"I\":\n",
        "            current_chunk += \" \" + token\n",
        "        last_iob = named_entity_tag_array[token_index][0]\n",
        "    chunk_array.append(current_chunk)\n",
        "    return chunk_array, chunk_label_array, total_chunks, effective_class_counts,"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYMV6HEza25I"
      },
      "source": [
        "### **TASK 1:**\n",
        "\n",
        "* Write a function to group recognized named entities using noun_chunks method of spaCy. Analyze the groups in terms of most frequent combinations (i.e. NER types that go together).\n",
        "\n",
        "We have alist of sentences as string (input) and a list of lists where the internal lists have as elements the label of the entities belonging to the same chunk noun (output).\n",
        "\n",
        "OPERATION PRINCIPLE: 2 STEPS\n",
        "\n",
        "* [doc.noun_chunks]. Creation of a list with label check on all tokens, if this first loop doesn't match labels or if it has already been detected, the function jumps to the next token. At the end of the process each token is added to the Chunk dictionary as a key and its value is a list containing as first element the list of the entity belonging to the Token Chunk and as second element the piece containing the key token.\n",
        "\n",
        "* [doc.ents]. If the first element of the entity B is one of the dictionary keys and the chunk to which it belongs has not yet been added to the list of the grouped entity, the list of all the entity labels of its noun piece is added to the group_ent list and its chunk is added to the chunk_done list to not be repeated when you find the other tokens belonging to that chunk.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATWWmLHNOq0T"
      },
      "source": [
        "def grouping_entities(text):\n",
        "    chunks=dict()                       #creating the dictionary containing sentences\n",
        "    chunk_done=[]                       #list to archive blocks of entities already added to the final output\n",
        "    group_ent=[]                        #list to store the future output\n",
        "    for sentence in text:\n",
        "        doc=nlp_spacy(sentence)         #get a document\n",
        "        for chunk in doc.noun_chunks:\n",
        "            l=[]\n",
        "            for c in chunk:\n",
        "                if c.ent_type_!=\"\" and c.ent_iob_=='B':\n",
        "                    l.append(c.ent_type_)\n",
        "            for ch in chunk:\n",
        "                chunks[ch]=[l, chunk]\n",
        "        for ent in doc.ents:\n",
        "            if ent[0] in chunks.keys() and chunks[ent[0]][1] not in chunk_done:\n",
        "                group_ent.append(chunks[ent[0]][0])\n",
        "                chunk_done.append(chunks[ent[0]][1])\n",
        "            elif ent[0] not in chunks.keys():\n",
        "                group_ent.append([ent[0].ent_type_])\n",
        "    return group_ent                                   #list with the grouped entities\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8utNfzadexUq"
      },
      "source": [
        "### **TASK 2:**\n",
        "\n",
        "* One of the possible post-processing steps is to fix segmentation errors. Write a function that extends the entity span to cover the full noun-compounds. Make use of compound dependency relation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXLLFSkYNNfg"
      },
      "source": [
        "def expand_entity_with_compound(sentence):\n",
        "    doc = nlp_standard(sentence)\n",
        "    ents = doc.ents\n",
        "    idx_to_tokenindex_map = {}\n",
        "    token_ent_pair_array= []\n",
        "    token_to_change = []\n",
        "    for token_index, token in enumerate(doc):\n",
        "        idx_to_tokenindex_map[token.idx] = token_index\n",
        "        if token.dep_ != \"compound\":\n",
        "            is_first = True\n",
        "            is_first_child = True\n",
        "            for child in token.children:\n",
        "                if child.dep_ == \"compound\" and child.idx < token.idx:\n",
        "                    is_first = False\n",
        "                    if is_first_child:\n",
        "                        token_ent_pair_array[idx_to_tokenindex_map[child.idx]] = (child.text, \"B-\" + token.ent_type_)\n",
        "                    else:\n",
        "                        token_ent_pair_array[idx_to_tokenindex_map[child.idx]] = (child.text, \"I-\" + token.ent_type_)\n",
        "                    is_first_child = False\n",
        "            if is_first:\n",
        "                ent_iob_ = \"O\"\n",
        "                if token.ent_iob_ != \"O\":\n",
        "                    ent_iob_ = token.ent_iob_ + \"-\"\n",
        "                token_ent_pair_array.append((token.text, ent_iob_ + token.ent_type_))\n",
        "            else:\n",
        "                token_ent_pair_array.append((token.text, \"I-\" + token.ent_type_))\n",
        "        else:\n",
        "            if token.head.idx < token.idx:\n",
        "                head_ent_type = token_ent_pair_array[idx_to_tokenindex_map[token.head.idx]][1][2:]\n",
        "                token_ent_pair_array.append((token.text, \"I-\" + head_ent_type))\n",
        "            else:\n",
        "                token_ent_pair_array.append(())\n",
        "    \n",
        "    return token_ent_pair_array"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi8O-ssGhxV5"
      },
      "source": [
        "### **Results:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9581aj4N72O"
      },
      "source": [
        "print()\n",
        "print(\"Task 0.1 results:\")\n",
        "conll_data = read_corpus_conll(\"./conll2003/test.txt\")\n",
        "print(\"Token classification:\", token_level_performance(conll_data))\n",
        "\n",
        "print()\n",
        "print(\"Task 0.2 results:\")\n",
        "effective_class_counts, recognized_class_counts, chunk_counter, recognized_chunk_counter = chunk_level_performance(conll_data)\n",
        "print(\"total chunk rateo:\", (recognized_chunk_counter/chunk_counter))\n",
        "print(\"class ORG rateo:\", recognized_class_counts[\"ORG\"] / effective_class_counts[\"ORG\"])\n",
        "print(\"class LOC rateo:\", recognized_class_counts[\"LOC\"] / effective_class_counts[\"LOC\"])\n",
        "\n",
        "\n",
        "test_sentence = \"Apple's Steve Jobs died in 2011 in Palo Alto, California.\"\n",
        "print()\n",
        "print(\"Task 1 results:\")\n",
        "print(grouping_entities(test_sentence))\n",
        "\n",
        "print()\n",
        "print(\"Task 2 Results:\")\n",
        "print(expand_entity_with_compound(test_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}